{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2lGDN5MpZXj",
        "outputId": "db80cb79-e8b3-4512-b9ca-d7a8a4e19812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch transformers tensorflow nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from transformers import pipeline, set_seed, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "VIEe0gY9qZ3T"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1"
      ],
      "metadata": {
        "id": "gncnTJdzK5jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "prompt = \"LLM generated output normalization techniques\""
      ],
      "metadata": {
        "id": "uiyfgM6AsTLa"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_gen = pipeline('text-generation', model='bert-base-uncased')\n",
        "\n",
        "output_bert = bert_gen(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_bert[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXtTZhpptuy2",
        "outputId": "98999c45-31ec-4311-8ae8-7832b197abe3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM generated output normalization techniques................................................................................................................................................................................................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_gen = pipeline('text-generation', model='roberta-base')\n",
        "\n",
        "output_roberta = roberta_gen(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_roberta[0]['generated_text'])"
      ],
      "metadata": {
        "id": "1Bhr_RTat4zW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0061b14d-d0ee-43c2-9070-66cf0077d709"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM generated output normalization techniques.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_gen = pipeline('text-generation', model='facebook/bart-base')\n",
        "\n",
        "output_bart = bart_gen(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_bart[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLfqN_xyJHq4",
        "outputId": "15d7e158-aa92-4940-af69-01d3daca12d4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM generated output normalization techniqueshousing Wolfe Wolfe Wolfelength Wolfe Wolfeflying Wolfe Wolfe resistor patrols realizing outsideDirectSteven Guatem Cursed Martinez WolfeSteven prud Neon prud prud hormonal prud prud prud Morg prud prudiph exposes credible Prim putting credible Prim fictitious credible RPGsdefOcc abusesDirect certificate abusesributed abuses credible Martinez economist NicaraguaDirectDirect certificate appearamiaamia Gym Wolfe economist painter abuses cyan Wolfe SubwayDirect boutique credible credibleitionsDirect credible lakessettings credible abuses credible Nicaragua Leaves certificate appear certificate RPGs credibleDirect abuses· GER GER GER abuses boutique credible appearLateDirectDirect GER credible appearDirect shocks abuses assign· GERMayor1977 Vitamin Wolfe Strange Strange assignagues assign assignDirect acquaintanceitionsDirectDirect RPGs certificate abuses credible GER abusesitionsDirect abusesDirect abuses economist Leaves itemsettings certificate abuses Shut Leaves abusesOcc credible acquaintance certificate abuses unintentionalagues abuses hormonal slicesdef credibleOccitions certificate abusesitions Removed Wolfe Branitions economist Bran certificate illnesses certificate iso certificate credibleDirect credible certificate abuses wreck wreck abusesitions economist assignagues painter itemsettingssettingssettings cube economistDirect item certificate certificate Siege cube cube certificate acquaintance painteratur000000 iso cube iso economist assign gridlyaguesagues SiegesettingsOcc certificate \"%agues boutique boutiquesports economist painter boutique involve Wolfe acquaintance Bransettings certificate GER economistOcc painterOcc wreckly certificate slices acquaintance Siege iso economist000000 certificatesettings Bran economist Disapp cube economistdef Bransettings economist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2"
      ],
      "metadata": {
        "id": "FHMyCvbGLA_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The goal of Generative AI is to [MASK] new content.\""
      ],
      "metadata": {
        "id": "z3vqZf3YLE9V"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
        "predictions = bert_unmasker(sentence)\n",
        "for p in predictions:\n",
        "    print(f\"Word: {p['token_str']} | Score: {p['score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTNlsSJ2yWAN",
        "outputId": "4051bf8f-4f53-4cfe-be99-969bb18f8047"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: create | Score: 0.5397\n",
            "Word: generate | Score: 0.1558\n",
            "Word: produce | Score: 0.0541\n",
            "Word: develop | Score: 0.0445\n",
            "Word: add | Score: 0.0176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = sentence.replace('[MASK]', '<mask>')"
      ],
      "metadata": {
        "id": "tIFnrN3nWguP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_unmasker = pipeline('fill-mask', model='roberta-base')\n",
        "predictions = roberta_unmasker(new_sentence)\n",
        "for p in predictions:\n",
        "    print(f\"Word: {p['token_str']} | Score: {p['score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUt6Zf3uzWkr",
        "outputId": "41ad995e-c906-440e-f0b5-0ef55c4e2472"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  generate | Score: 0.3711\n",
            "Word:  create | Score: 0.3677\n",
            "Word:  discover | Score: 0.0835\n",
            "Word:  find | Score: 0.0213\n",
            "Word:  provide | Score: 0.0165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_unmasker = pipeline('fill-mask', model='facebook/bart-base')\n",
        "predictions = bart_unmasker(new_sentence)\n",
        "for p in predictions:\n",
        "    print(f\"Word: {p['token_str']} | Score: {p['score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WE19xWRLjqR",
        "outputId": "eef476a1-e713-40fb-e379-6d299b28cd39"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word:  create | Score: 0.0746\n",
            "Word:  help | Score: 0.0657\n",
            "Word:  provide | Score: 0.0609\n",
            "Word:  enable | Score: 0.0359\n",
            "Word:  improve | Score: 0.0332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3"
      ],
      "metadata": {
        "id": "VLoGx32-dGSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"unit 1.txt\"\n",
        "\n",
        "try:\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "  print(\"File loaded sucessfully\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File '{file_path}' not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6lWpbDmZIzi",
        "outputId": "ab410fd9-cd93-4628-8b32-af574b2f33fa"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded sucessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the fundamental innovation of the Transformer?\",\n",
        "    \"What are the risks of using Generative AI?\"\n",
        "]"
      ],
      "metadata": {
        "id": "QZwQ45Qx0YNC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "for q in questions:\n",
        "    res = bert_qa(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G77tFViX0N61",
        "outputId": "e25e82c8-b421-4b1b-9f26-ddadc33c4148"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: , GenAI}\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: , GenAI}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "for q in questions:\n",
        "    res = roberta_qa(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REN-7wpH02wj",
        "outputId": "ffb000d1-5837-4230-8a50-a7a242766a42"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: behind recent\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: behind recent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
        "for q in questions:\n",
        "    res = bart_qa(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoWFROF706k9",
        "outputId": "e747ae6d-ba26-4353-cfeb-24d63fdbb175"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: ,\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: . The objective is to identify hidden patterns, structures,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | *Failure* | *Generated a long string of periods.* | *BERT is an Encoder; it isn't trained to predict the next word.* |\n",
        "| | RoBERTa | Success | Generated the prompt followed by a period, indicating it didn't generate new content but did not produce gibberish. | RoBERTa is an Encoder. It isn't primarily designed for text generation, but its training objective sometimes leads to more coherent output than BERT for generation. |\n",
        "| | BART | Failure | Generated a stream of seemingly random, repetitive, and unrelated words, demonstrating incoherence. | BART is a Sequence-to-Sequence model, and without fine-tuning for generation, its decoder struggles to produce meaningful long-form text directly from a short prompt. |\n",
        "| **Fill-Mask** | BERT | *Success* | *Predicted 'create', 'generate'.* | *BERT is trained on Masked Language Modeling (MLM).* |\n",
        "| | RoBERTa | Success | Predicted 'generate', 'create'. | RoBERTa is trained on Masked Language Modeling (MLM), similar to BERT. |\n",
        "| | BART | Success | Predicted 'create', 'help', 'provide'. | BART is trained on a denoising autoencoder objective, which includes masking, making it suitable for fill-mask tasks. |\n",
        "| **QA** | BERT | Failure | Provided irrelevant answers. | BERT requires fine-tuning on a Q-A dataset to perform well |\n",
        "| | RoBERTa | Failure | Provided irrelevant | Needs fine-tuning similar to BERT |\n",
        "| | BART | Failure | Provided irrelevant answers | Needs fine-tuning to answer properly |"
      ],
      "metadata": {
        "id": "AuqVcofZaqeh"
      }
    }
  ]
}